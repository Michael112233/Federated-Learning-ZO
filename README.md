# Federated Learning Project

## 提前标注

**options.py和baseline_main.py在本次实验中并没有用到，
仅只是本人编写代码时进行参照的内容以及可能之后会用到的代码，
可以忽略**

## algorithm.py

定义算法类，目前存储了FedAvg算法和Zeroth-grad算法

算法类的存储有固定格式，每个算法类都会固定分成四个函数

1. `__init__` 用于超参数和模型，数据集的存储，每一次的实验都务必要对模型进行重置

2. `update_weight` 用于模拟单个客户端的更新

3. `average` 用于聚合

4. `alg_run`  负责整个算法的运行

2023.10.16补充： 既然我们的算法本身是零阶的，那么我们选择算法的时候也应该选择零阶的算法，比如说在这里我们需要将常规的FedAvg算法中求梯度的部分改成使用有限微分法求梯度的零阶算法

## test.py

牛顿法验证代码，牛顿法精度较高，通过对比牛顿法的损失函数值和我们写的损失函数值即可判断代码的准确性

如果把超参数的值调高，在评估次数为3000*数据集大小的时候，两个数据集使用零阶优化的算法计算的损失函数与牛顿法的相差不大于0.03，所以可以认为如果算法能够跑起来，得到的损失函数结果应该是没有问题的

## models.py

存储了模型信息，在这里我们目前只使用了逻辑回归这一模型，在条件允许的情况下，可以使用多个模型来辅助实验

模型主要完成求梯度`def grad()`和计算损失函数`def loss()`两个功能

2023.10.16补充：
1. 根据老师建议，无梯度的算法就直接包含在算法类的`update_client`函数中，没有在这里封装
2. 需要注意稀疏矩阵与实矩阵相乘的问题（区分`np.dot(x, W)`以及`x.dot(W)`）

2023.10.23补充：
1. 为了代码简洁，合并针对mnist和rcv1的模型

## sampling.py

封装数据类，数据类中存储了X, Y的值，训练集和测试集的划分，小批量采样等步骤

同时，在`sampling.py`中，我们定义了get_mnist以及get_rcv1函数，用于读取mnist和rcv1数据集

如果需要读取新的数据集，可以把这部分读取数据集的程序封装在这个地方

## federated_main.py





## 后续任务（ddl 10.30）
1. 加快调参代码运行速度，加入并行化程序
2. 程序选择最优eta参数
3. 完成文档编写
4. 控制每种情况的最大评估次数保持一致，方便后续进行对比
----
5. 对比算法（10.30后）
